{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Module_5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOJRkH2XKOZ8TWuZ3GjVbW6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HernanMagallanes/TensorFlow_course/blob/main/Module_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 5: Deep Computer Vision - Convolutional Neural Networks\n"
      ],
      "metadata": {
        "id": "CB3QDsZ-JPL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will learn how to peform *image classification and object detection/recognition* using deep computer vision with something called a **convolutional neural network**.\n",
        "\n",
        "The goal of our convolutional neural networks will be to classify and detect images or specific objects from within the image. We will be using image data as our features and a label for those images as our label or output.\n",
        "\n",
        "Core concepts:\n",
        "- Image Data\n",
        "- Convolutional Layer\n",
        "- Pooling Layer\n",
        "- CNN Architectures\n"
      ],
      "metadata": {
        "id": "XaHwgHI8gXjl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image data\n",
        "\n",
        "3 dimensions: \n",
        "- image height\n",
        "- image width\n",
        "- color channels (rgb pixels/layer)\n",
        "\n",
        "##Convolutional Neural Network\n",
        "\n",
        "Each convolutional neural network is made up of one or many convolutional layers. These layers are different than the *dense* layers we have seen previously. Their goal is to find patterns from within images that can be used to classify the image or parts of it.\n",
        "\n",
        "The **fundemental difference** between a dense layer and a convolutional layer is that dense layers detect patterns globally while convolutional layers detect patterns locally. \n",
        "\n",
        "When we have a densly connected layer each node in that layer sees all the data from the previous layer. This means that this layer is looking at all the information and is only capable of analyzing the data in a global capacity. Our convolutional layer however will not be densly connected, this means it can detect local patterns using part of the input data to that layer.\n",
        "\n",
        "**Dense Layer:** A dense layer will consider the ENTIRE image.\n",
        "\n",
        "**Convolutional Layer:** The convolutional layer will look at specific parts of the image."
      ],
      "metadata": {
        "id": "cBvdtxMNj2Hr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vU6HI6VenlLH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Multiple Convolutional Layers\n",
        "In our models it is quite common to have more than one convolutional layer. Even the basic example we will use in this guide will be made up of 3 convolutional layers. These layers work together by increasing complexity and abstraction at each subsequent layer. \n",
        "\n",
        "The first layer might be responsible for picking up edges and short lines, while the second layer will take as input these lines and start forming shapes or polygons. Finally, the last layer might take these shapes and determine which combiantions make up a specific image."
      ],
      "metadata": {
        "id": "dcomdg_vnndb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature Maps\n",
        "\n",
        " This term simply stands for a 3D tensor with two spacial axes (width and height) and one depth axis. \n",
        "\n",
        "  Our convolutional layers take feature maps as their input and return a new feature map that reprsents the prescence of spcific filters from the previous feature map. These are what we call *response maps*.\n",
        "\n",
        "##Layer Parameters\n",
        "\n",
        "A convolutional layer is defined by two key parameters: filters and sample Size\n",
        "\n",
        "####**Filters**\n",
        "A filter is a m x n pattern of pixels that we are looking for in an image. The number of filters in a convolutional layer reprsents how many patterns each layer is looking for and what the depth of our response map will be. \n",
        "\n",
        "If we are looking for 32 different patterns/filters than our output feature map (aka the response map) will have a depth of 32. Each one of the 32 layers of depth will be a matrix of some size containing values indicating if the filter was present at that location or not.\n",
        "\n",
        "####**Sample Size**\n",
        "This isn't really the best term to describe this, but each convolutional layer is going to examine n x m blocks of pixels in each image. Typically, we'll consider 3x3 or 5x5 blocks. In the example above we use a 3x3 \"sample size\". This size will be the same as the size of our filter. \n",
        "\n",
        "Our layers work by sliding these filters of n x m pixels over every possible position in our image and populating a new feature map/response map indicating whether the filter is present at each location. \n",
        "\n"
      ],
      "metadata": {
        "id": "NEPbeijZnnYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Borders and Padding\n",
        "The more mathematical of you may have realized that if we slide a filter of let's say size 3x3 over our image well consider less positions for our filter than pixels in our input. Look at the example below. \n",
        "\n",
        "This means our response map will have a slightly smaller width and height than our original image. This is fine but sometimes we want our response map to have the same dimensions. We can accomplish this by using something called *padding*.\n",
        "\n",
        "**Padding** is simply the addition of the appropriate number of rows and/or columns to your input data such that each pixel can be centered by the filter.\n",
        "\n",
        "##Strides\n",
        "In the previous sections we assumed that the filters would be slid continously through the image such that it covered every possible position. This is common but sometimes we introduce the idea of a **stride** to our convolutional layer. The stride size reprsents how many rows/cols we will move the filter each time. These are not used very frequently so we'll move on.\n",
        "\n",
        "Larger stride, smaller output feature map is going to be. add more padding.\n",
        "\n",
        "##Pooling\n",
        "You may recall that our convnets are made up of a stack of convolution and pooling layers.\n",
        "\n",
        "The idea behind a pooling layer is to downsample our feature maps and reduce their dimensions. They work in a similar way to convolutional layers where they extract windows from the feature map and return a response map of the max, min or average values of each channel. Pooling is usually done using windows of size 2x2 and a stride of 2. This will reduce the size of the feature map by a factor of two and return a response map that is 2x smaller."
      ],
      "metadata": {
        "id": "O8_d4x-ynv7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iou8zkNYg85v",
        "outputId": "054847c2-542b-4799-b46d-c3e9b86e0991"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x  # this line is not required unless you are in a notebook`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  LOAD AND SPLIT DATASET\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "300YBIGfg823",
        "outputId": "f5c42c8c-4cd0-4207-9bc5-483e9a8f57c3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n",
            "170508288/170498071 [==============================] - 11s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at a one image\n",
        "IMG_INDEX = 7\n",
        "\n",
        "plt.imshow(train_images[IMG_INDEX] ,cmap=plt.cm.binary)\n",
        "plt.xlabel(class_names[train_labels[IMG_INDEX][0]])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "S8fdyXkyg81T",
        "outputId": "ae6865bf-69e9-4c8e-c4bf-1c6d42942b0c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CNN Architecture\n",
        "A common architecture for a CNN is a stack of Conv2D and MaxPooling2D layers followed by a few denesly connected layers. To idea is that the stack of convolutional and maxPooling layers extract the features from the image. Then these features are flattened and fed to densly connected layers that determine the class of an image based on the presence of features.\n",
        "\n",
        "We will start by building the **Convolutional Base**."
      ],
      "metadata": {
        "id": "wuW6nphcut1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Layer 1\n",
        "# The input shape of our data will be 32, 32, 3 and \n",
        "# we will process 32 filters of size 3x3 over our input data \n",
        "\n",
        "# Layer 2\n",
        "# This layer will perform the max pooling operation using 2x2 samples and a stride of 2.\n",
        "\n",
        "# Other Layers\n",
        "# The next set of layers do very similar things but take as input the feature map from the previous layer.\n",
        "# They also increase the frequency of filters from 32 to 64. \n",
        "# We can do this as our data shrinks in spacial dimensions as it passed through the layers,\n",
        "#  meaning we can afford (computationally) to add more depth.\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "model.summary() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w375shQgg8zm",
        "outputId": "97419686-2293-4af2-883c-81290f20b61a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 30, 30, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 15, 15, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 13, 13, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 6, 6, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 4, 4, 64)          36928     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 56,320\n",
            "Trainable params: 56,320\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Adding Dense Layers\n",
        "\n",
        "Now we need to take these extracted features and add a way to classify them. This is why we add the following layers to our model.\n",
        "\n"
      ],
      "metadata": {
        "id": "MSkU807mv125"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "model.summary() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yutPCRwg8t0",
        "outputId": "5fa1e004-7552-416c-ad75-2dfc817b0373"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 30, 30, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 15, 15, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 13, 13, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 6, 6, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 4, 4, 64)          36928     \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 64)                65600     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 122,570\n",
            "Trainable params: 122,570\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_images, train_labels, epochs=4, \n",
        "                    validation_data=(test_images, test_labels))"
      ],
      "metadata": {
        "id": "QoYdhExMg8mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "print(test_acc)"
      ],
      "metadata": {
        "id": "6pjw70kHg8jH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working with small datasets\n",
        "\n",
        "In the situation where you don't have millions of images it is difficult to train a CNN from scratch that performs very well. This is why we will learn about a few techniques we can use to train CNN's on small datasets of just a few thousand images. \n",
        "\n",
        "###Data Augmentation\n",
        "\n",
        "This is simply performing random transofrmations on our images so that our model can generalize better. These transformations can be things like compressions, rotations, stretches and even color changes. \n",
        "\n",
        "Fortunately, keras can help us do this. Look at the code below to an example of data augmentation.\n"
      ],
      "metadata": {
        "id": "_f3ECphihc0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data augmentation\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# creates a data generator object that transforms images\n",
        "datagen = ImageDataGenerator(\n",
        "rotation_range=40,\n",
        "width_shift_range=0.2,\n",
        "height_shift_range=0.2,\n",
        "shear_range=0.2,\n",
        "zoom_range=0.2,\n",
        "horizontal_flip=True,\n",
        "fill_mode='nearest')\n",
        "\n",
        "# pick an image to transform\n",
        "test_img = train_images[20]\n",
        "img = image.img_to_array(test_img)  # convert image to numpy arry\n",
        "img = img.reshape((1,) + img.shape)  # reshape image\n",
        "\n",
        "i = 0\n",
        "\n",
        "for batch in datagen.flow(img, save_prefix='test', save_format='jpeg'):  # this loops runs forever until we break, saving images to current directory with specified prefix\n",
        "    plt.figure(i)\n",
        "    plot = plt.imshow(image.img_to_array(batch[0]))\n",
        "    i += 1\n",
        "    if i > 4:  # show 4 images\n",
        "        break\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "may-UzbAg8ZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pretrained model"
      ],
      "metadata": {
        "id": "WUq7AgiQhnc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "keras = tf.keras"
      ],
      "metadata": {
        "id": "gvYXXInFg8VZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_label_name = metadata.features['label'].int2str  # creates a function object that we can use to get labels\n",
        "\n",
        "# display 2 images from the dataset\n",
        "for image, label in raw_train.take(5):\n",
        "  plt.figure()\n",
        "  plt.imshow(image)\n",
        "  plt.title(get_label_name(label))"
      ],
      "metadata": {
        "id": "iuzN4_3tJP5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data proprocessing\n",
        "\n",
        "IMG_SIZE = 160 # All images will be resized to 160x160\n",
        "\n",
        "def format_example(image, label):\n",
        "  \"\"\"\n",
        "  returns an image that is reshaped to IMG_SIZE\n",
        "  \"\"\"\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image = (image/127.5) - 1\n",
        "  image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "  return image, label"
      ],
      "metadata": {
        "id": "AwnDFKXrJP7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = raw_train.map(format_example)\n",
        "validation = raw_validation.map(format_example)\n",
        "test = raw_test.map(format_example)"
      ],
      "metadata": {
        "id": "p-MnS8PGhuHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image, label in train.take(2):\n",
        "  plt.figure()\n",
        "  plt.imshow(image)\n",
        "  plt.title(get_label_name(label))"
      ],
      "metadata": {
        "id": "mkynntIghuEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "SHUFFLE_BUFFER_SIZE = 1000\n",
        "\n",
        "train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "validation_batches = validation.batch(BATCH_SIZE)\n",
        "test_batches = test.batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "WR54VLHshuBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for img, label in raw_train.take(2):\n",
        "  print(\"Original shape:\", img.shape)\n",
        "\n",
        "for img, label in train.take(2):\n",
        "  print(\"New shape:\", img.shape)"
      ],
      "metadata": {
        "id": "wESy-e5-gQ4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Picking a pretrained model\n",
        "\n",
        "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
        "\n",
        "# Create the base model from the pre-trained model MobileNet V2\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')"
      ],
      "metadata": {
        "id": "lDmK8FkEh9kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.summary()"
      ],
      "metadata": {
        "id": "OsQhJlvmh9gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image, _ in train_batches.take(1):\n",
        "   pass\n",
        "\n",
        "feature_batch = base_model(image)\n",
        "print(feature_batch.shape)"
      ],
      "metadata": {
        "id": "YghOA5Dgh9eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freezing the base\n",
        "base_model.trainable = False\n",
        "\n",
        "base_model.summary()"
      ],
      "metadata": {
        "id": "KsYJD_J5h9c8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding our classifier\n",
        "\n",
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()"
      ],
      "metadata": {
        "id": "d1933ThTh9Xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_layer = keras.layers.Dense(1)"
      ],
      "metadata": {
        "id": "W71w8k1fh9Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "  base_model,\n",
        "  global_average_layer,\n",
        "  prediction_layer\n",
        "])"
      ],
      "metadata": {
        "id": "ndSvMOzzh9Qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model\n",
        "\n",
        "base_learning_rate = 0.0001\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "nlbt1oZtiUPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can evaluate the model right now to see how it does before training it on our new images\n",
        "initial_epochs = 3\n",
        "validation_steps=20\n",
        "\n",
        "loss0,accuracy0 = model.evaluate(validation_batches, steps = validation_steps)"
      ],
      "metadata": {
        "id": "0RHSAk07iUK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can train it on our images\n",
        "history = model.fit(train_batches,\n",
        "                    epochs=initial_epochs,\n",
        "                    validation_data=validation_batches)\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "YmkD7_idiUH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"dogs_vs_cats.h5\")  # we can save the model and reload it at anytime in the future\n",
        "new_model = tf.keras.models.load_model('dogs_vs_cats.h5')"
      ],
      "metadata": {
        "id": "axA0g2PniUFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0BUh7Hw3id7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ayUSBj7pid4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-7pQzlCiidzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "r2R-X0b-idwo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}